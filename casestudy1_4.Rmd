---
title: "Case 1 Take 4"
author: "Ian Hua, InHee Ho, Sonia Xu"
date: "September 25, 2017"
output:
  word_document: default
  pdf_document: default
  html_document: default
---
#Transitioning from the Linear Mixed Effects Model
In our previous report, we attempted to handle the non-linearity of dose 1 and dose 2 by taking their logarithms and modeling as polynomials with a degree of 2. Overall, our previous final model fitted the data well according to the qq-plot and residual vs. fitted graph. However, using second-degree polynomials of the logarithmized values of dose 1 and dose 2 did not fully capture the uneven distribution of dose 1 and dose 2 against the response, blotted uterus weight. We can better fit the model using kernel regression and clustering. We present below our updated final model that applies kernel estimations to dose 1 and dose 2 and transforms body weight into a categorical variable via k-means clustering, and how the model better predicts our response.

#Exploratory Data Analysis
We continued from our previous report with our choices of explanatory variables. We detected significant heteroscedasticity in our response variable (blotted uterus weight) between labs, and took their logarithms for more consistent variance. Furthermore, protocols can have different effects on the response depending on the labs. We capture such difference by fitting random effects proto|lab. Finally, to capture the effects of doses of chemical 1 and 2 across all labs as well as their variation between labs, we added dose 1 and dose 2 as both fixed and random effects. However, we used different methods to better capture the distributions of dose 1, dose 2, and body weight.

##Capturing the Non-Linearity of Dose 1 and Dose 2
In order to capture the non-linearity of the distribution of dose 1 and dose 2, we decided to use kernel regression instead of modeling with second-degree polynomials of logarithmized values. We use 4 knots for dose 1 and 3 knots for dose 2.


```{r include = F}
#Read in the data
library(dplyr)
library(lme4)
library(ggplot2)
library(magrittr)
dat <- read.table("case1.txt", header = T, stringsAsFactors = F, na.strings = ".", colClasses=c("character","character","numeric","numeric","numeric","numeric","numeric","numeric"))
dat <-  dat %>% filter(complete.cases(blot)) 
```

```{r include = F}
#creates a kernel distribution for each covariate
#be careful of random samples--Big X can't have NAs
kern_distribution <- function(x,knots = 4) {
  ### s: #controls how wide the kernels are, s should be half the distance between two knots 
  ### tau: decides where the breaks are based on the number of knots
  ###knots: number of splits/peaks in the data
  ##x and y are the data
  
  tau <- seq(min(x), max(x), length.out = knots)
  s <- diff(tau)[1]/2
  X <- matrix(0, nrow=length(x),ncol=knots)
  for(i in 1:knots) {
    X[,i] <- x * dnorm(x, tau[i], s)
  }
  return(X)
}

#smoothing out the graph by predicting values for the fitted data
#same function as the kern distribution, but this time it is smoother
smooth_graph <- function(x,knots = 4) {
  tau <- seq(min(x), max(x), length.out = knots)
  s <- diff(tau)[1]/2
  x.predict <- seq(min(x), max(x), length.out=length(x))
  X.predict <- matrix(0, nrow=length(x.predict),ncol = knots)
  for(i in 1:knots) {
    X.predict[,i] <- x.predict * dnorm(x.predict, tau[i], s)
  }
  return(X.predict)
}
```

```{r echo = F, warning= F}

#attach(dat) ##attaches the data so we can just run through with the variable names

m2 <- lmer(log(blot + 1) ~ (1 + proto + kern_distribution(dose1, knots = 4) + kern_distribution(dose2, knots = 3)|lab) + kern_distribution(dose1, knots = 4) + kern_distribution(dose2, knots = 3) , data = dat)  #X + Z + R + S + random effect
#summary(m2)
```

```{r echo = F}
plot(dat$dose1,log(dat$blot + 1), col = "red", main = "Fitted Kernels for Dose 1", ylab = "log(blot + 1)", xlab = "dose 1") #plots the true data
lines(sort(dat$dose1), fitted(m2)[order(dat$dose1)]) #shows how well the model fits the true data
#abline(v = tau)

plot(dat$dose2, log(dat$blot + 1), col = "red", main = "Fitted Kernels for Dose 2", ylab = "log(blot + 1)", xlab = "dose 1")
lines(sort(dat$dose2), fitted(m2)[order(dat$dose2)])
#abline(v = tau)

```
##Transforming Body into a Categorical Variable
```{r, echo=F}
ggplot(data = dat, aes(x=body, y=blot, color=lab))+geom_point(alpha=0.5)
```

Looking at the scatterplot above between body and blotted uterus weight, we observe two distinct groups of body weight. These groups are non-linear, so it would be best to distinguish them in an unsupervised fashion. K-means clustering with 3 groups resulted in the best segmentation of the data, and these groups (1,2,3) replaced the continuous variable of body weight for the final model. Below is a plot of the 3 different clusters.
```{r echo = F}
library(cluster)
set.seed(20)
bodyCluster <- kmeans(dat[, c(6,8)], 3, nstart = 20)

bodyCluster$cluster <- as.factor(bodyCluster$cluster)
ggplot(dat, aes(body, blot, color = bodyCluster$cluster)) + geom_point()

#Splits the body weight into 3 bins...
##add it back to the data & replace body with these 3 groups

dat$bodyClust <- bodyCluster$cluster
```

```{r include = F}
#smooth version
m3 <- lmer(log(blot + 1) ~ (1 + proto|lab) + smooth_graph(dose1, knots = 3) + smooth_graph(dose2, knots = 3) + smooth_graph(body, knots = 3), data = dat) #X + Z + R + S + random effect
summary(m3)

```

#The Updated Final Model
Thus, our updated final model still has the log of blotted uterus weights as the response variable; protocol, dose 1, and dose 2 as random effects between labs; and dose 1, dose 2, and body weight as fixed effects across all population. But dose 1, dose 2, and body weight now reflect the transformations as noted above.
```{r}
m4 <- lmer(log(blot + 1) ~ (1 + proto + kern_distribution(dose1, knots = 4) + kern_distribution(dose2, knots = 3)|lab) + kern_distribution(dose1, knots = 4) + kern_distribution(dose2, knots = 3) + bodyClust, data = dat) 
#X + Z + R + S + random effect
#summary(m4)
```
#Checking Assumptions
```{r}
plot(m4, main = 'Residuals vs. Fitted', xlab = "Fitted", ylab = "Residuals")
```
Looking at the Residuals vs. Fitted plot, the points exhibit homoscedascity, and overall, the points are randomly and evenly distributed above and below the horizontal axis. This implies that the data should be modeled linearly. 

```{r}
qqnorm(resid(m4))
```
Checking the normality assumption via the QQ-plot, the points overall follow a straight diagonal trend, which reaffirms that the points are normally distributed.

#Final Model Goodness of Fit
```{r cache = T, echo = F}
R5 <- NULL
SD <- NULL
for(i in 1: 100) {
  samp <- sample(dim(dat)[1], dim(dat)[1]/10)
  test <- dat[samp,]
  train <- dat[-samp,]
  ufinmod <- lmer(log(blot + 1) ~ (1 + proto + kern_distribution(dose1, knots = 4) + kern_distribution(dose2, knots = 3)|lab) + kern_distribution(dose1, knots = 4) + kern_distribution(dose2, knots = 3) + bodyClust, data = dat) 
  p_finmod <- predict(ufinmod, test)
  SD <- rbind(SD, sd(log(test$blot + 1)))
  R5 <- rbind(R5, RMSE(p_finmod, log(test$blot+1)))
}
```

By fitting dose 1 and dose 2 via kernel regression and clustering body into 3 segments, this updated final model produced better predictive results for the train vs. test data. The previous final model had a root mean squared error (RMSE) of 0.2339 after 100 iterations. This final model had a root mean squared error of `r mean(R5)`after 100 iterations.


