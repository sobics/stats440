---
title: "Case 1 Take 4"
author: "Ian Hua, InHee Ho, Sonia Xu"
date: "September 25, 2017"
output: html_document
---

##An Attempt at a Non-linear Model
Instead of modeling dose 1 and dose 2 as polynomials with a degree of 2, we decided to capture the non-linearity of dose 1 and dose 2 via kernel regression. We also noted that body exhibited non-linear trends, so we also decided to model this feature via kernel regression.

#Read in the data
```{r}
library(dplyr)
library(lme4)
library(ggplot2)
library(magrittr)
dat <- read.table("case1.txt", header = T, stringsAsFactors = F, na.strings = ".", colClasses=c("character","character","numeric","numeric","numeric","numeric","numeric","numeric"))
dat <-  dat %>% filter(complete.cases(blot)) 
```

```{r}
#creates a kernel distribution for each covariate
#be careful of random samples--Big X can't have NAs
kern_distribution <- function(x,knots = 4) {
  ### s: #controls how wide the kernels are, s should be half the distance between two knots 
  ### tau: decides where the breaks are based on the number of knots
  ###knots: number of splits/peaks in the data
  ##x and y are the data
  
  tau <- seq(min(x), max(x), length.out = knots)
  s <- diff(tau)[1]/2
  X <- matrix(0, nrow=length(x),ncol=knots)
  for(i in 1:knots) {
    X[,i] <- x * dnorm(x, tau[i], s)
  }
  return(X)
}

#smoothing out the graph by predicting values for the fitted data
#same function as the kern distribution, but this time it is smoother
smooth_graph <- function(x,knots = 4) {
  tau <- seq(min(x), max(x), length.out = knots)
  s <- diff(tau)[1]/2
  x.predict <- seq(min(x), max(x), length.out=length(x))
  X.predict <- matrix(0, nrow=length(x.predict),ncol = knots)
  for(i in 1:knots) {
    X.predict[,i] <- x.predict * dnorm(x.predict, tau[i], s)
  }
  return(X.predict)
}
```
#Kernel Distributions
```{r}

#attach(dat) ##attaches the data so we can just run through with the variable names

m2 <- lmer(log(blot + 1) ~ (1 + proto|lab) + kern_distribution(dose1, knots = 3) + kern_distribution(dose2, knots = 3) + kern_distribution(body, knots = 3), data = dat) #X + Z + R + S + random effect
summary(m2)


plot(dat$dose1,log(dat$blot + 1)) #plots the true data
lines(sort(dat$dose1), fitted(m2)[order(dat$dose1)]) #shows how well the model fits the true data
abline(v = tau)

plot(dat$dose2, log(dat$blot + 1))
lines(sort(dat$dose2), fitted(m2)[order(dat$dose2)])
#abline(v = tau)


plot(dat$body, log(dat$blot + 1))
lines(sort(dat$body), fitted(m2)[order(dat$body)])

```

#smooth version
```{r}
m3 <- lmer(log(blot + 1) ~ (1 + proto|lab) + smooth_graph(dose1, knots = 3) + smooth_graph(dose2, knots = 3) + smooth_graph(body, knots = 3), data = dat) #X + Z + R + S + random effect
summary(m3)
y.predict <- predict(m3, dat)
  #cbind(1, cbind(smooth_graph(dat$dose1, dat$blot), ) %*% coef(m2) #predict function is easier // but cbind(X.predicts)
plot(dat$dose2,log(dat$blot+1))
lines(sort(dat$dose2), fitted(m3)[order(dat$dose2)])
```
##categorical variable for body?
#maybe we can split body into 2 bins, and use that to predict body weight
```{r}
library(cluster)
set.seed(20)
bodyCluster <- kmeans(dat[, c(6,8)], 3, nstart = 20)

bodyCluster$cluster <- as.factor(bodyCluster$cluster)
ggplot(dat, aes(body, blot, color = bodyCluster$cluster)) + geom_point()

#Splits the body weight into 3 bins...
##add it back to the data & replace body with these 3 groups

dat$bodyClust <- bodyCluster$cluster
m4 <- lmer(log(blot + 1) ~ (1 + proto|lab) + kern_distribution(dose1, knots = 3) + kern_distribution(dose2, knots = 3) + bodyClust, data = dat) #X + Z + R + S + random effect
summary(m4)
```


