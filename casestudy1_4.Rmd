---
title: "Case 1 Take 4"
author: "Ian Hua, InHee Ho, Sonia Xu"
date: "September 25, 2017"
output:
  pdf_document: default
  html_document: default
  word_document: default
---
#Transitioning from the Linear Mixed Effects Model
Previously, we attempted to model the non-linearity of dose 1 and dose 2 by modeling these features as polynomials with a degree of 2. However, modeling these features as polynomials did not fully capture their distribution against the response, blot. Our previous final model, overall, fit the data well according to the qq-plot and residual vs. fitted graph. However, we show below that we can better fit a model by fitting dose 1 and dose 2 via kernel estimations and by transforming body into a categorical variable via k-means clustering (k = 3). Below is an explanation of our updated final model. 

##Capturing the Non-Linearity of Dose 1 and Dose 2
Instead of modeling dose 1 and dose 2 as polynomials with a degree of 2, we decided to capture the non-linearity of dose 1 and dose 2 via kernel regression. Below are plots of how the kernel distribution fits against the true data for dose 1 and dose 2.


```{r include = F}
#Read in the data
library(dplyr)
library(lme4)
library(ggplot2)
library(magrittr)
dat <- read.table("case1.txt", header = T, stringsAsFactors = F, na.strings = ".", colClasses=c("character","character","numeric","numeric","numeric","numeric","numeric","numeric"))
dat <-  dat %>% filter(complete.cases(blot)) 
```

```{r include = F}
#creates a kernel distribution for each covariate
#be careful of random samples--Big X can't have NAs
kern_distribution <- function(x,knots = 4) {
  ### s: #controls how wide the kernels are, s should be half the distance between two knots 
  ### tau: decides where the breaks are based on the number of knots
  ###knots: number of splits/peaks in the data
  ##x and y are the data
  
  tau <- seq(min(x), max(x), length.out = knots)
  s <- diff(tau)[1]/2
  X <- matrix(0, nrow=length(x),ncol=knots)
  for(i in 1:knots) {
    X[,i] <- x * dnorm(x, tau[i], s)
  }
  return(X)
}

#smoothing out the graph by predicting values for the fitted data
#same function as the kern distribution, but this time it is smoother
smooth_graph <- function(x,knots = 4) {
  tau <- seq(min(x), max(x), length.out = knots)
  s <- diff(tau)[1]/2
  x.predict <- seq(min(x), max(x), length.out=length(x))
  X.predict <- matrix(0, nrow=length(x.predict),ncol = knots)
  for(i in 1:knots) {
    X.predict[,i] <- x.predict * dnorm(x.predict, tau[i], s)
  }
  return(X.predict)
}
```

```{r echo = F, warning= F}

#attach(dat) ##attaches the data so we can just run through with the variable names

m2 <- lmer(log(blot + 1) ~ (1 + proto + kern_distribution(dose1, knots = 4) + kern_distribution(dose2, knots = 3)|lab) + kern_distribution(dose1, knots = 4) + kern_distribution(dose2, knots = 3) , data = dat)  #X + Z + R + S + random effect
#summary(m2)
```

```{r echo = F}
plot(dat$dose1,log(dat$blot + 1), col = "red", main = "Fitted Kernels for Dose 1", ylab = "log(blot + 1)", xlab = "dose 1") #plots the true data
lines(sort(dat$dose1), fitted(m2)[order(dat$dose1)]) #shows how well the model fits the true data
#abline(v = tau)

plot(dat$dose2, log(dat$blot + 1), col = "red", main = "Fitted Kernels for Dose 2", ylab = "log(blot + 1)", xlab = "dose 1")
lines(sort(dat$dose2), fitted(m2)[order(dat$dose2)])
#abline(v = tau)

```
##Transforming Body into a Categorical Variable
Looking at the scatterplot between body and blotted uterus weight, there are two distinct groups for body weight. These groups are non-linear, so it would be best to distinguish them in an unsupervised fashion. K-means clustering with 3 groups resulted in the best segmentation of the data, and these groups (1,2,3) replaced the continuous variable of body weight for the final model. Below is a plot of the 3 different clusters.
```{r echo = F}
library(cluster)
set.seed(20)
bodyCluster <- kmeans(dat[, c(6,8)], 3, nstart = 20)

bodyCluster$cluster <- as.factor(bodyCluster$cluster)
ggplot(dat, aes(body, blot, color = bodyCluster$cluster)) + geom_point()

#Splits the body weight into 3 bins...
##add it back to the data & replace body with these 3 groups

dat$bodyClust <- bodyCluster$cluster
```

```{r include = F}
#smooth version
m3 <- lmer(log(blot + 1) ~ (1 + proto|lab) + smooth_graph(dose1, knots = 3) + smooth_graph(dose2, knots = 3) + smooth_graph(body, knots = 3), data = dat) #X + Z + R + S + random effect
summary(m3)
y.predict <- predict(m3, dat)
  #cbind(1, cbind(smooth_graph(dat$dose1, dat$blot), ) %*% coef(m2) #predict function is easier // but cbind(X.predicts)
plot(dat$dose2,log(dat$blot+1))
lines(sort(dat$dose2), fitted(m3)[order(dat$dose2)])
```

#The Updated Final Model
Thus, the updated final model still contains the log of blotted uterus weights as the response, transformations of dose 1, dose 2, and body as noted above, and random effects for protocol, dose 1, and dose 2 between labs.
```{r}
m4 <- lmer(log(blot + 1) ~ (1 + proto + kern_distribution(dose1, knots = 5) + kern_distribution(dose2, knots = 3)|lab) + kern_distribution(dose1, knots = 5) + kern_distribution(dose2, knots = 3) + bodyClust, data = dat) 
#X + Z + R + S + random effect
#summary(m4)
```
#Checking Assumptions
```{r}
plot(m4, main = 'Residuals vs. Fitted', xlab = "Fitted", ylab = "Residuals")
```
Looking at the Residuals vs. Fitted plot, the points exhibit homoscedascity, and overall, the points are randomly and evenly distributed above and below the horizontal axis. This implies that the data should be modeled linearly. 

```{r}
qqnorm(resid(m4))
```
Checking the normality assumption via the QQ-plot, the points overall follow a straight diagonal trend, which reaffirms that the points are normally distributed.

```{r cache = T, echo = F}
R5 <- NULL
SD <- NULL
for(i in 1: 100) {
  samp <- sample(dim(dat)[1], dim(dat)[1]/10)
  test <- dat[samp,]
  train <- dat[-samp,]
  ufinmod <- lmer(log(blot + 1) ~ (1 + proto + kern_distribution(dose1, knots = 5) + kern_distribution(dose2, knots = 3)|lab) + kern_distribution(dose1, knots = 5) + kern_distribution(dose2, knots = 3) + bodyClust, data = dat) 
  p_finmod <- predict(ufinmod, test)
  SD <- rbind(SD, sd(log(test$blot + 1)))
  R5 <- rbind(R5, RMSE(p_finmod, log(test$blot+1)))
}
```

By fitting dose 1 and dose 2 via kernel regression and clustering body into 3 segments, this updated final model produced better predictive results for the train vs. test data. The previous final model had a root mean squared error (RMSE) of 0.2339 after 100 iterations. This final model had a root mean squared error of after 100 iterations.


#Final Model Goodness of Fit
