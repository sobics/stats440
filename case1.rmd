---
title: "Case Study Pt. 2"
author: "Ian Hua, InHee Ho, Sonia Xu"
date: "September 9, 2017"
output: html_document
---

#Case Study 1: Write Up 2
```{r include = F}
library(dplyr)
library(lme4)
library(ggplot2)
#read in the data
###plot multiple ggplots in one place  obtained from http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

####

RMSE <- function(x,y) {return(sqrt(mean((y-x)^2)))} 
dat <- read.table("case1.txt", header = T, stringsAsFactors = F, na.strings = ".", colClasses=c("character","character","numeric","numeric","numeric","numeric","numeric","numeric"))
dat <-  dat %>% filter(complete.cases(blot)) %>% mutate(wblot = blot/body, wwet = wet/body)
```
To continue our exploration of modelling the data, we decided to split the cleaned dataset from our first write-up into test and training data. We will fit the model on the training data to see how well it fits our test data. Our test data is 1/10 of the full dataset. Another goodness-of-fit measurement we will use is Root Mean Squared Error (RMSE), which measures the difference between the predicted response from a model and the true response the dataset. The lower the RMSE, the better the model fits the data. However, although a good indicator, it is important to note that relying solely on RMSE to assess the goodness-of-fit of a model is not enough; We must rely on multiple measurements like AIC, Adjusted R-Squared, ANOVA tests, etc to compare models. We sampled/fit the test and training data for 100 iterations for each model in an effort to normalize the performance of the models. By understanding how well the models predict the blotted uterus weight, we can see if there is improvement from utilizing more complex models with fixed and random effects in comparison to the simple linear model.

```{r include = F}
set.seed(3)
samp <- sample(dim(dat)[1], dim(dat)[1]/10)
test <- dat[samp,]
train <- dat[-samp,]
```
We believe that the best model predicts the most similar distribution of the test dataset compared to the true test dataset.
Thus, the best models should be the most similar to this quantile:
```{r}
quantile(test$wblot)
```

#Simple Linear Model
```{r}
lm1 <- lm(wblot ~ 1+ poly(dose1,2) + poly(dose2,2), data = dat)
par(mfrow=c(2,2))
plot(lm1)
```

```{r include = F, warnings = F, cache = T}
library(Metrics)
R1 <- NULL
for(i in 1: 100) {
  samp <- sample(dim(dat)[1], dim(dat)[1]/10)
  test <- dat[samp,]
  train <- dat[-samp,]
  lm1 <- lm(wblot ~ 1+ poly(dose1,2) + poly(dose2,2), data = train)
  p_lm1 <- predict(lm1, test)
  R1 <- rbind(R1, rmse(p_lm1, test$wblot))
}
```
Checking the basic assumptions of normality for the simple linear model, we can clearly see problems in the model. Comparing its prediction accuracy to the true dataset, we obtain `r quantile(p_lm1)`, which has a similar distribution mean. However, the tails of the predicted test dataset are wider than the true test dataset, which suggests that this model can be improved. Again, using the test data, the predicted response's Root Mean Squared Error (RMSE) is `r mean(R1)`.

#Proposed Model from Write-up 1
```{r}
fm1 <- lmer(wblot ~ 1 + (proto|lab) + body + poly(dose1,2) + poly(dose2,2), dat)
summary(fm1)
```

```{r echo = F, warnings = F, cache = T}
R2 <- NULL
for(i in 1: 100){
  samp <- sample(dim(dat)[1], dim(dat)[1]/10)
  test <- dat[samp,]
  train <- dat[-samp,]
  fm1 <- lmer(wblot ~ 1 + (proto|lab) + body + poly(dose1,2) + poly(dose2,2), train)
  p_fm1 <- predict(fm1, test)
  R2 <- rbind(R2, RMSE(p_fm1, test$wblot))
}
ggplot(data = test, aes(p_fm1, wblot)) + geom_point(aes(colour = lab)) + geom_smooth() + geom_line(aes(colour=lab), linetype=5)
```
Based on our intuitive assumptions from Case Study 1, above is our proposed model from write-up 1. The plot demonstrates how well our proposed model's prediction of the test data fits the true test data--a linear trend indicates perfect prediction. Overall, the predictive power of our intuitive model seems to work pretty well. The distribution of our proposed model more closely follows that of the true dataset compared to the Simple Linear Model, since the RMSE (`r mean(R2)`) is lower by `r mean(R1)-mean(R2)` in comparison to the baseline model. 

#Minimum Model
However, we were informed by a more experienced Statistician (our TA) about the necessity to model the data with a minimum model that contained random effects for protocol, dose1, and dose 2 for each lab because there is high variance in the effect of dose1 and dose2 for each lab.

The plots below demonstrate these differences:
```{r echo = F}
mean_dat1 <- dat %>% group_by(lab, dose1) %>% summarise(wblot = mean(wblot))
d1_graph <- ggplot(data = mean_dat1, aes(x = dose1, y = wblot, color = lab)) + geom_line() + ggtitle("Dose 1")
mean_dat2 <- dat %>% group_by(lab, dose2) %>% summarise(wblot = mean(wblot))
d2_graph <- ggplot(data = mean_dat2, aes(x = dose2, y = wblot, color = lab)) + geom_line() + ggtitle("Dose 2")

multiplot(d1_graph, d2_graph, cols = 2)
```
In order to capture these random effects of dose1 and dose2, below is our minimum model.
```{r}
#dose response curve
minmod <- lmer(wblot ~ (1 + proto + poly(dose1,2) + poly(dose2,2)| lab), dat) #minimum model
summary(minmod)
```

```{r include=FALSE, warnings = F, cache = T}
R3 <- NULL
for(i in 1: 100) {
  samp <- sample(dim(dat)[1], dim(dat)[1]/10)
  test <- dat[samp,]
  train <- dat[-samp,]
  minmod <- lmer(wblot ~ (1 + proto + poly(dose1,2) + poly(dose2,2)|lab), train)
  p_minmod <- predict(minmod, test)
  R3 <- rbind(R3, RMSE(p_minmod, test$wblot))
}
```
The Minimum Model performs better than our Proposed Model, with an average RMSE of `r mean(R3)` after 100 iterations, which is `r mean(R2)-mean(R3)` lower than our Proposed Model. Furthermore, the AIC for the Minimum Model (`r summary(minmod)$AIC`) is lower and thus better than the Proposed Model's AIC (`r summary(fm1)$AIC`), which implies that this model is an improvement from the last. 

With the Minimum Model as our baseline, we continued to explore different models. After much exploration, this was our Final Model:

#Final Model
```{r warnings= F}
finmod <- lmer(wblot ~ (1 + poly(dose1,2) + poly(dose2,2) |lab)+poly(dose1,2)+poly(dose2,2)+poly(body,2)+(proto*lab), dat) #final model
summary(finmod)

```

```{r include = F, cache = T, warning = F} 
R4 <- NULL
for(i in 1: 100) {
  samp <- sample(dim(dat)[1], dim(dat)[1]/10)
  test <- dat[samp,]
  train <- dat[-samp,]
  finmod <- lmer(wblot ~ (1 + poly(dose1,2) + poly(dose2,2) |lab)+poly(dose1,2)+poly(dose2,2)+poly(body,2)+(proto*lab), train)
  p_finmod <- predict(finmod, test)
  R4 <- rbind(R4, RMSE(p_finmod, test$wblot))
}

```
Overall, our Final Model's average RMSE (`r mean(R4)`) is again lower than the Minimum Model's average RMSE after 100 iterations. Comparing our Final Model's AIC against the Minimum Models' AIC, again, our Final Model has a lower AIC (`r summary(finmod)$AIC`) than our Minimum Model (`r summary(minmod)$AIC`). Furthermore, when conducting an ANOVA test between our Final Model and the Minimum Model, the Final Model is significantly better.

```{r warning = F}
anova(finmod, minmod)
```
Below is a plot of how well our final model predicts the true data. Overall, the trend is linear, which indicates a relatively accurate prediction.
```{r echo = F}
ggplot(data = test, aes(p_finmod, wblot)) + geom_smooth() #+ geom_line(aes(colour=lab), linetype=5)

```

After exploring and comparing multiple models, our final model looks like this:

$$y_{ij} = \mu_i + \beta_{i,dose1}\cdot dose1_{ij}^2 + \beta_{i,dose2}\cdot dose2_{ij}^2 + \gamma_{dose1}\cdot dose1_{ij}^2 + \gamma_{dose2}\cdot dose2_{ij}^2 + \gamma_{body}\cdot body_{ij}^2 + \gamma_{inter}\cdot proto_{ij}\cdot lab_{ij} +\epsilon_{ij}$$ where $\epsilon_{ij} \sim N(0,\nu^2)$ and $\mu_i \sim N(\mu, \sigma^2)$$.

We noticed that adding dose1 and dose2 as both fixed and random effects better fits the data. The fixed effect captures the population effects of dose1 and dose2 across all labs and random effect captures the variation in the effects of dose1 and dose2 between labs. Similarly, we also assumed that different types of protocol have a different effect for each labs. After fitting the model with the interaction between protocol and lab (proto*lab) and with the random effects of protocol on each labs (proto|lab) respectively, we observed that the former better fits the actual dataset (see Appendix). We decided to transform body weight, dose1, and dose2 to a polynomial with a degree of 2 because the scatterplot indicated a nonlinear trend between body weight and the response, the weighted blot. 

#Conclusion

From this final model, we can conclude the following:

1. Body weight: Body weight squared is proportional to the weighted blot

2. Dose 1: Dose 1 squared is proportional to the weighted blot - dose 1 functions as an agonist.

3. Dose 2: Dose 2 squared is inversely proportional to the weighted blot - dose 2 functions as an antagonist.


#Appendix

This ANOVA test demonstrates why we chose to use proto*lab as a fixed effect instead of a random effect.
```{r, warning=F}

fm1 <- lmer(wblot ~ (1 + poly(dose1,2) + poly(dose2,2) |lab) + poly(dose1,2) + poly(dose2,2) + poly(body,2) + (proto*lab), dat) # interaction

fm2 <- lmer(wblot ~ (1 + proto + poly(dose1,2) + poly(dose2,2) |lab) + poly(dose1,2) + poly(dose2,2) + poly(body,2), dat) # random effect

anova(fm1, fm2)
```

These plots illustrate the distributions of the predicted response, wblot. The more similar the distribution to the original wblot, the more accurate the prediction and the better the model. 
```{r}
test_p <- ggplot(data = test, aes(x = wblot)) + geom_histogram() + lims(x = c(0,4)) + theme_bw() + ggtitle("True WBlot Distribution")
lm_p <- ggplot(test,aes(p_lm1)) + geom_histogram() + lims(x = c(0,4)) + theme_bw() + ggtitle("Simple Linear Model")
min_p <- ggplot(test,aes(p_minmod)) + geom_histogram() + lims(x = c(0,4)) + theme_bw() + ggtitle("Minimum Model")
fin_p <- ggplot(test,aes(p_finmod)) + geom_histogram() + lims(x = c(0,4)) + theme_bw() + ggtitle("Final Model")
multiplot(test_p, lm_p, min_p, fin_p, cols = 2)
```


#Contributions
Sonia Xu - goodness-of-fit for every model (RMSE, test/training), wrote case study 2, explored models
InHee Ho - explored various models, edited & added to case study 2
Ian Hua - explored various models